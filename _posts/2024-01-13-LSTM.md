---
layout: post
title: "LSTM: Long Short-Term Memory"
date: 2025-01-13 10:00:00 +0000
categories: [Concept, RNN]
tags: [LSTM]
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

When dealing with sequential data like time series, language, or videos, traditional neural networks struggle to capture long-term dependencies. Enter **Long Short-Term Memory (LSTM)** — a specialized type of Recurrent Neural Network (RNN) designed to learn both **short-term patterns** and **long-term relationships**.

Let’s break down LSTM, step by step.

---

## What is an LSTM?

LSTM is a type of RNN that uses a **cell state** and **gates** to control the flow of information, allowing it to selectively remember or forget past data. This architecture helps LSTM solve the **vanishing gradient problem** that affects standard RNNs, enabling it to capture dependencies over long sequences.

### Key Concepts:
- **Cell state ($$c_t$$)**: Acts as the LSTM's "memory," carrying information across time steps.
- **Hidden state ($$h_t$$)**: Represents the output of the LSTM at a given time step.
- **Gates**: Mechanisms to control how information flows in and out of the cell.

![LSTM](articles_img/LSTM/LSTM.png){: style="width: 70%;" }

---

## Why Use LSTMs?
- **Capturing Long-Term Dependencies**: Retains relevant context over long sequences (e.g., understanding "I love ..." while processing "cats and dogs" in a sentence).
- **Handling Sequential Data**: Ideal for text, speech, time series, and video data.
- **Solving the Vanishing Gradient Problem**: Gates ensure stable learning over long sequences.

---

## Step-by-Step Breakdown of LSTM

LSTM processes data sequentially. For each time step $$t$$, it uses the current input $$x_t$$, the previous hidden state $$h_{t-1}$$, and the previous cell state $$c_{t-1}$$. Here’s how it works:

### 1. Forget Gate
The **forget gate** determines what information from the previous cell state ($$c_{t-1}$$) should be retained or discarded.

$$
    f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$

- $$\sigma$$: Sigmoid activation outputs values between $$0$$ (forget completely) and $$1$$ (keep fully).
- Output ($$f_t$$): Elementwise control of $$c_{t-1}$$.

### 2. Input Gate
The **input gate** decides what new information to add to the cell state.

$$
    i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

Simultaneously, the **candidate cell state** proposes new information to update the memory:

$$
    \tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

- $$\sigma$$: Controls how much of the candidate memory to use.
- $$\tanh$$: Ensures new information is scaled between $$[-1, +1]$$.

### 3. Update Cell State
The updated **cell state ($$c_t$$)** combines retained old memory and new information:

$$
    c_t = f_t \odot c_{t-1} + i_t \odot \tilde{C}_t
$$

- $$f_t \odot c_{t-1}$$: Retains relevant past information.
- $$i_t \odot \tilde{C}_t$$: Adds new relevant information.

### 4. Output Gate
The **output gate** determines what part of the updated cell state should be passed as the hidden state ($$h_t$$):

$$
    o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

The hidden state is:

$$
    h_t = o_t \odot \tanh(c_t)
$$

- $$\tanh(c_t)$$: Scales the updated cell state into $$[-1, +1]$$.
- $$o_t$$: Controls what part of the scaled cell state becomes the output.


---

## Applications of LSTMs

- **Natural Language Processing (NLP)**: Text generation, machine translation, sentiment analysis.
- **Speech Recognition**: Identifying patterns in speech over time.
- **Time Series Forecasting**: Stock price predictions, weather forecasting.
- **Video Analysis**: Understanding sequences of frames.

---

## Summary

- LSTM uses **gates** to control the flow of information, solving limitations of traditional RNNs.
- The **cell state** enables long-term memory, while the **hidden state** captures short-term dependencies.
- By learning when to remember, forget, and output information, LSTMs achieve outstanding results in sequential data tasks.

<iframe src="/articles_img/LSTM/LSTM.pdf" style="width:100%; height:600px; border:none;"></iframe>


---

